{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problema: Reducir  el tiempo de busqueda de un Robot a un item en un area que pertenece a una planta industrial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "# Environment\n",
    "# policy\n",
    "# reward\n",
    "# *render,  gym\n",
    "\n",
    "import os\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "    0 1 2 3 4\n",
    "    5 6 7 8 9\n",
    "    ....\n",
    "    .\n",
    "\"\"\"\n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self,num_actions: int):\n",
    "        self.n = num_actions\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n-1)\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    # X,Y plane: UP, DOWN, RIGHT, LEFT\n",
    "    seeker, goal = (0,0), (4,4)\n",
    "    info = {'seeker': seeker, 'goal': goal}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(25)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.seeker = (0,0)        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        return 5*self.seeker[0]+self.seeker[1]\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        return self.seeker == self.goal\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   # move down\n",
    "            self.seeker = (min(self.seeker[0]+1,4), self.seeker[1])\n",
    "        elif action == 1: # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1]-1,0))\n",
    "        elif action == 2: # move up\n",
    "            self.seeker = (max(self.seeker[0]-1,0), self.seeker[1])\n",
    "        elif action == 3: # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1]+1,4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        return self.get_observation(), self.get_reward(), self.is_done(), self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        # os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        clear_output()\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "        \n",
    "import numpy as np\n",
    "class Policy:\n",
    "    def __init__(self, env):\n",
    "        self.state_action_table = [[0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)] # observation_space = 25\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "    def get_action(self, state, explore=True, epsilon=0.1 ):\n",
    "        if explore and (random.uniform(0,1) < epsilon):\n",
    "            return self.action_space.sample()\n",
    "        print(\"state:\", state)\n",
    "        print(\"table state:\", self.state_action_table[state])\n",
    "        print(\"argmax: \", np.argmax(self.state_action_table[state]))\n",
    "        return np.argmax(self.state_action_table[state])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feda6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| | | | | |\n",
      "| |S| | |G|\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     experiences\u001b[38;5;241m.\u001b[39mappend([state, action, reward, next_state])    \n\u001b[1;32m     11\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(experiences)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time \n",
    "env = Environment()\n",
    "experiences = []\n",
    "policy = Policy(env)\n",
    "state = env.reset()\n",
    "while not env.is_done():\n",
    "    # random_action = env.action_space.sample()\n",
    "    action = policy.get_action(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    experiences.append([state, action, reward, next_state])    \n",
    "    state = next_state\n",
    "    time.sleep(0.5)\n",
    "    env.render()\n",
    "        \n",
    "\n",
    "print(experiences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bigdata-2022-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee0e340b373adaa70299d42cd1cb59b0a3467f40584d69eef1fc62bec46809f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
